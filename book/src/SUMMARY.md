# LLM Serving in a Week

[Preface](./preface.md)
[Setting Up the Environment](./setup.md)

---

- [Week 1: From Matmul to Text](./week1-overview.md)
    - [Attention and Multi-Head Attention](./week1-01-attention.md)
    - [Positional Encodings and RoPE](./week1-02-positional-encodings.md)
    - [Grouped/Multi Query Attention](./week1-03-gqa.md)
    - [RMSNorm and MLP](./week1-04-rmsnorm-and-mlp.md)
    - [The Qwen2 Model](./week1-05-qwen2-model.md)
    - [Generating the Response](./week1-06-generate-response.md)
    - [Sampling and Preparing for Week 2](./week1-07-sampling-prepare.md)
- [Week 2: Tiny vLLM](./week2-overview.md)
    - [Key-Value Cache](./week2-01-kv-cache.md)
    - [Quantized Matmul (2 Days)]()
    - [Flash Attention (2 Days)]()
    - [Chunked Prefill]()
    - [Continuous Batching]()
- [Week 3: Serving]()

---

[Glossary Index](./glossary.md)
