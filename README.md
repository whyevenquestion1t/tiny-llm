# tiny-llm - LLM Serving in a Week

[![CI (main)](https://github.com/skyzh/tiny-llm/actions/workflows/main.yml/badge.svg)](https://github.com/skyzh/tiny-llm/actions/workflows/main.yml)

A course on LLM serving using MLX for system engineers. The codebase
is solely (almost!) based on MLX array/matrix APIs without any high-level neural network APIs, so that we
can build the model serving infrastructure from scratch and dig into the optimizations.

The goal is to learn the techniques behind efficiently serving a large language model (e.g., Qwen2 models).

Why MLX: nowadays it's easier to get a macOS-based local development environment than setting up an NVIDIA GPU.

Why Qwen2: this was the first LLM I've interacted with -- it's the go-to example in the vllm documentation. I spent some time looking at the vllm source code and built some knowledge around it.

## Book

The tiny-llm book is available at [https://skyzh.github.io/tiny-llm/](https://skyzh.github.io/tiny-llm/). You can follow the guide and start building.

## Community

You may join skyzh's Discord server and study with the tiny-llm community.

[![Join skyzh's Discord Server](book/src/discord-badge.svg)](https://skyzh.dev/join/discord)

## Roadmap

Week 1 is complete. Week 2 is in progress.

| Week + Chapter | Topic                                                       | Code | Test | Doc |
| -------------- | ----------------------------------------------------------- | ---- | ---- | --- |
| 1.1            | Attention                                                   | âœ…    | âœ…   | âœ…  |
| 1.2            | RoPE                                                        | âœ…    | âœ…   | âœ…  |
| 1.3            | Grouped Query Attention                                     | âœ…    | âœ…   | âœ…  |
| 1.4            | RMSNorm and MLP                                             | âœ…    | âœ…   | âœ…  |
| 1.5            | Load the Model                                              | âœ…    | âœ…   | âœ…  |
| 1.6            | Generate Responses (aka Decoding)                           | âœ…    | âœ…   | âœ…  |
| 1.7            | Sampling                                                    | âœ…    | âœ…   | âœ…  |
| 2.1            | Key-Value Cache                                             | âœ…    | ðŸš§   | ðŸš§  |
| 2.2            | Quantized Matmul and Linear - CPU                           | âœ…    | ðŸš§   | ðŸš§  |
| 2.3            | Quantized Matmul and Linear - GPU                           | âœ…    | ðŸš§   | ðŸš§  |
| 2.4            | Flash Attention 2 - CPU                                     | âœ…    | ðŸš§   | ðŸš§  |
| 2.5            | Flash Attention 2 - GPU                                     | âœ…    | ðŸš§   | ðŸš§  |
| 2.6            | Continuous Batching                                         | âœ…    | ðŸš§   | ðŸš§  |
| 2.7            | Chunked Prefill                                             | âœ…    | ðŸš§   | ðŸš§  |
| 3.1            | Paged Attention - Part 1                                    | ðŸš§    | ðŸš§   | ðŸš§  |
| 3.2            | Paged Attention - Part 2                                    | ðŸš§    | ðŸš§   | ðŸš§  |
| 3.3            | MoE (Mixture of Experts)                                    | ðŸš§    | ðŸš§   | ðŸš§  |
| 3.4            | Speculative Decoding                                        | ðŸš§    | ðŸš§   | ðŸš§  |
| 3.5            | Prefill-Decode Separation (requires two Macintosh devices)  | ðŸš§    | ðŸš§   | ðŸš§  |
| 3.6            | Parallelism                                                 | ðŸš§    | ðŸš§   | ðŸš§  |
| 3.7            | AI Agent     / Tool Calling                                 | ðŸš§    | ðŸš§   | ðŸš§  |

Other topics not covered: quantized/compressed kv cache, prefix/prompt cache; sampling, fine tuning; smaller kernels (softmax, silu, etc)
